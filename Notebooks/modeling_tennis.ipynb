{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25a042a3-0546-4e8c-933e-79aadb1124ee",
   "metadata": {},
   "source": [
    "# 2. Modeling – Predicting Whether the Stronger Player Wins\n",
    "\n",
    "In this notebook, we build machine learning models to predict whether the **stronger player (higher-ranked)** wins a tennis match.\n",
    "\n",
    "**Target variable:**\n",
    "\n",
    "- `stronger_win = 1` → higher-ranked player wins (expected outcome)\n",
    "- `stronger_win = 0` → lower-ranked player wins (upset)\n",
    "\n",
    "We use the cleaned dataset generated in the EDA notebook: `clean_matches_tennis.csv`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bcd576b3-f5a4-42bb-8f18-b986c118fbec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# !{sys.executable} -m pip install xgboost\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f68ec6f-0d1d-4f20-9be4-2ddb6c4e7e84",
   "metadata": {},
   "source": [
    "## 2.1 Load Cleaned Dataset\n",
    "\n",
    "We load the cleaned dataset `clean_matches_tennis.csv` created in the EDA notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a8ba5714-95e7-4429-a0cf-88e5b3cc60d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: (2760, 24)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tourney_id</th>\n",
       "      <th>tourney_name</th>\n",
       "      <th>surface</th>\n",
       "      <th>draw_size</th>\n",
       "      <th>tourney_level</th>\n",
       "      <th>tourney_date</th>\n",
       "      <th>match_num</th>\n",
       "      <th>winner_name</th>\n",
       "      <th>winner_id</th>\n",
       "      <th>winner_rank</th>\n",
       "      <th>winner_rank_points</th>\n",
       "      <th>winner_age</th>\n",
       "      <th>loser_name</th>\n",
       "      <th>loser_id</th>\n",
       "      <th>loser_rank</th>\n",
       "      <th>loser_rank_points</th>\n",
       "      <th>loser_age</th>\n",
       "      <th>stronger_rank</th>\n",
       "      <th>weaker_rank</th>\n",
       "      <th>rank_gap_abs</th>\n",
       "      <th>points_diff</th>\n",
       "      <th>age_diff</th>\n",
       "      <th>best_of</th>\n",
       "      <th>stronger_win</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-M020</td>\n",
       "      <td>Brisbane</td>\n",
       "      <td>Hard</td>\n",
       "      <td>32</td>\n",
       "      <td>A</td>\n",
       "      <td>20181231</td>\n",
       "      <td>300</td>\n",
       "      <td>Kei Nishikori</td>\n",
       "      <td>105453</td>\n",
       "      <td>9.0</td>\n",
       "      <td>3590.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>Daniil Medvedev</td>\n",
       "      <td>106421</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1977.0</td>\n",
       "      <td>22.8</td>\n",
       "      <td>9.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1613.0</td>\n",
       "      <td>6.2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-M020</td>\n",
       "      <td>Brisbane</td>\n",
       "      <td>Hard</td>\n",
       "      <td>32</td>\n",
       "      <td>A</td>\n",
       "      <td>20181231</td>\n",
       "      <td>299</td>\n",
       "      <td>Daniil Medvedev</td>\n",
       "      <td>106421</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1977.0</td>\n",
       "      <td>22.8</td>\n",
       "      <td>Jo-Wilfried Tsonga</td>\n",
       "      <td>104542</td>\n",
       "      <td>239.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>33.7</td>\n",
       "      <td>16.0</td>\n",
       "      <td>239.0</td>\n",
       "      <td>223.0</td>\n",
       "      <td>1777.0</td>\n",
       "      <td>-10.9</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-M020</td>\n",
       "      <td>Brisbane</td>\n",
       "      <td>Hard</td>\n",
       "      <td>32</td>\n",
       "      <td>A</td>\n",
       "      <td>20181231</td>\n",
       "      <td>298</td>\n",
       "      <td>Kei Nishikori</td>\n",
       "      <td>105453</td>\n",
       "      <td>9.0</td>\n",
       "      <td>3590.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>Jeremy Chardy</td>\n",
       "      <td>104871</td>\n",
       "      <td>40.0</td>\n",
       "      <td>1050.0</td>\n",
       "      <td>31.8</td>\n",
       "      <td>9.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>2540.0</td>\n",
       "      <td>-2.8</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-M020</td>\n",
       "      <td>Brisbane</td>\n",
       "      <td>Hard</td>\n",
       "      <td>32</td>\n",
       "      <td>A</td>\n",
       "      <td>20181231</td>\n",
       "      <td>297</td>\n",
       "      <td>Jo-Wilfried Tsonga</td>\n",
       "      <td>104542</td>\n",
       "      <td>239.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>33.7</td>\n",
       "      <td>Alex De Minaur</td>\n",
       "      <td>200282</td>\n",
       "      <td>31.0</td>\n",
       "      <td>1298.0</td>\n",
       "      <td>19.8</td>\n",
       "      <td>31.0</td>\n",
       "      <td>239.0</td>\n",
       "      <td>208.0</td>\n",
       "      <td>-1098.0</td>\n",
       "      <td>13.9</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-M020</td>\n",
       "      <td>Brisbane</td>\n",
       "      <td>Hard</td>\n",
       "      <td>32</td>\n",
       "      <td>A</td>\n",
       "      <td>20181231</td>\n",
       "      <td>296</td>\n",
       "      <td>Daniil Medvedev</td>\n",
       "      <td>106421</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1977.0</td>\n",
       "      <td>22.8</td>\n",
       "      <td>Milos Raonic</td>\n",
       "      <td>105683</td>\n",
       "      <td>18.0</td>\n",
       "      <td>1855.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>122.0</td>\n",
       "      <td>-5.2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  tourney_id tourney_name surface  draw_size tourney_level  tourney_date  \\\n",
       "0  2019-M020     Brisbane    Hard         32             A      20181231   \n",
       "1  2019-M020     Brisbane    Hard         32             A      20181231   \n",
       "2  2019-M020     Brisbane    Hard         32             A      20181231   \n",
       "3  2019-M020     Brisbane    Hard         32             A      20181231   \n",
       "4  2019-M020     Brisbane    Hard         32             A      20181231   \n",
       "\n",
       "   match_num         winner_name  winner_id  winner_rank  winner_rank_points  \\\n",
       "0        300       Kei Nishikori     105453          9.0              3590.0   \n",
       "1        299     Daniil Medvedev     106421         16.0              1977.0   \n",
       "2        298       Kei Nishikori     105453          9.0              3590.0   \n",
       "3        297  Jo-Wilfried Tsonga     104542        239.0               200.0   \n",
       "4        296     Daniil Medvedev     106421         16.0              1977.0   \n",
       "\n",
       "   winner_age          loser_name  loser_id  loser_rank  loser_rank_points  \\\n",
       "0        29.0     Daniil Medvedev    106421        16.0             1977.0   \n",
       "1        22.8  Jo-Wilfried Tsonga    104542       239.0              200.0   \n",
       "2        29.0       Jeremy Chardy    104871        40.0             1050.0   \n",
       "3        33.7      Alex De Minaur    200282        31.0             1298.0   \n",
       "4        22.8        Milos Raonic    105683        18.0             1855.0   \n",
       "\n",
       "   loser_age  stronger_rank  weaker_rank  rank_gap_abs  points_diff  age_diff  \\\n",
       "0       22.8            9.0         16.0           7.0       1613.0       6.2   \n",
       "1       33.7           16.0        239.0         223.0       1777.0     -10.9   \n",
       "2       31.8            9.0         40.0          31.0       2540.0      -2.8   \n",
       "3       19.8           31.0        239.0         208.0      -1098.0      13.9   \n",
       "4       28.0           16.0         18.0           2.0        122.0      -5.2   \n",
       "\n",
       "   best_of  stronger_win  \n",
       "0        3             1  \n",
       "1        3             1  \n",
       "2        3             1  \n",
       "3        3             0  \n",
       "4        3             1  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_PATH = \"./clean_matches_tennis.csv\"  # 根据你的路径调整，如果在上一级就用 ../Data/...\n",
    "\n",
    "if not os.path.exists(DATA_PATH):\n",
    "    raise FileNotFoundError(f\"Cannot find {DATA_PATH}. Make sure you ran the EDA notebook and saved this file.\")\n",
    "\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "print(\"Data shape:\", df.shape)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988eaaea-941f-4ff9-8ce0-0477d60144ed",
   "metadata": {},
   "source": [
    "## 2.2 Target Variable: `stronger_win`\n",
    "\n",
    "We confirm the distribution of the target:\n",
    "\n",
    "- 1 → stronger player wins  \n",
    "- 0 → upset (weaker player wins)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6bf53c8d-35ef-4e78-b283-987b8fc00a33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "stronger_win\n",
       "1    0.614493\n",
       "0    0.385507\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if \"stronger_win\" not in df.columns:\n",
    "    raise KeyError(\"Column 'stronger_win' not found. Make sure you used the EDA notebook definition.\")\n",
    "\n",
    "df[\"stronger_win\"].value_counts(normalize=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7063ef06-ac85-4e69-a481-4a8d8ece0f62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "stronger_win\n",
       "1    1696\n",
       "0    1064\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"stronger_win\"].value_counts()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "890c196e-8f7e-4555-a035-ba7ecc13032d",
   "metadata": {},
   "source": [
    "## 2.3 Feature Selection\n",
    "\n",
    "We focus on pre-match features that are realistically available **before** a match:\n",
    "\n",
    "**Numeric features:**\n",
    "- `rank_gap_abs` – absolute ranking gap (weaker_rank − stronger_rank)\n",
    "- `points_diff` – ranking points difference between winner and loser\n",
    "- `age_diff` – age difference between winner and loser\n",
    "\n",
    "**Categorical features:**\n",
    "- `surface` – court surface\n",
    "- `tourney_level` – tournament category\n",
    "- `best_of` – number of sets (3 or 5)\n",
    "\n",
    "**Target:**\n",
    "- `y = stronger_win`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c30c2b01-dfb3-45f1-9243-43d6a1aa2f5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2208, 5), (552, 5))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# numeric & categorical feature names\n",
    "numeric_features = [\"rank_gap_abs\",  \"age_diff\"]\n",
    "categorical_features = [\"surface\", \"tourney_level\", \"best_of\"]\n",
    "\n",
    "for col in numeric_features + categorical_features + [\"stronger_win\"]:\n",
    "    if col not in df.columns:\n",
    "        raise KeyError(f\"Missing required column: {col}\")\n",
    "\n",
    "X = df[numeric_features + categorical_features].copy()\n",
    "y = df[\"stronger_win\"].astype(int)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y,\n",
    ")\n",
    "\n",
    "X_train.shape, X_test.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4da2c5c-6177-429d-9531-85cfbf01a002",
   "metadata": {},
   "source": [
    "## 2.4 Preprocessing and Pipelines\n",
    "\n",
    "We use a `ColumnTransformer` to:\n",
    "\n",
    "- Pass numeric features as-is (tree-based models do not require scaling).\n",
    "- One-hot encode categorical features.\n",
    "\n",
    "Then we wrap this preprocessor with different classifiers:\n",
    "\n",
    "- Decision Tree\n",
    "- Random Forest\n",
    "- XGBoost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8a81f468-11fc-448a-83d4-c04bc60e1dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", \"passthrough\", numeric_features),\n",
    "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), categorical_features),\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5548d86-7a72-4a1c-aa76-4b510d3b4d99",
   "metadata": {},
   "source": [
    "## 2.5 Baseline Models: Decision Tree & Random Forest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0857a3e1-7da1-4174-9047-0819967d707c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Decision Tree ===\n",
      "Accuracy: 0.5253623188405797\n",
      "Confusion matrix:\n",
      " [[ 77 136]\n",
      " [126 213]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.38      0.36      0.37       213\n",
      "           1       0.61      0.63      0.62       339\n",
      "\n",
      "    accuracy                           0.53       552\n",
      "   macro avg       0.49      0.49      0.49       552\n",
      "weighted avg       0.52      0.53      0.52       552\n",
      "\n",
      "\n",
      "=== Random Forest ===\n",
      "Accuracy: 0.5670289855072463\n",
      "Confusion matrix:\n",
      " [[ 79 134]\n",
      " [105 234]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.43      0.37      0.40       213\n",
      "           1       0.64      0.69      0.66       339\n",
      "\n",
      "    accuracy                           0.57       552\n",
      "   macro avg       0.53      0.53      0.53       552\n",
      "weighted avg       0.56      0.57      0.56       552\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Decision Tree\n",
    "dt_clf = Pipeline(\n",
    "    steps=[\n",
    "        (\"preprocess\", preprocess),\n",
    "        (\"model\", DecisionTreeClassifier(max_depth=None, random_state=42)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "dt_clf.fit(X_train, y_train)\n",
    "y_pred_dt = dt_clf.predict(X_test)\n",
    "\n",
    "acc_dt = accuracy_score(y_test, y_pred_dt)\n",
    "cm_dt = confusion_matrix(y_test, y_pred_dt)\n",
    "\n",
    "print(\"=== Decision Tree ===\")\n",
    "print(\"Accuracy:\", acc_dt)\n",
    "print(\"Confusion matrix:\\n\", cm_dt)\n",
    "print(classification_report(y_test, y_pred_dt))\n",
    "\n",
    "\n",
    "# Random Forest\n",
    "rf_clf = Pipeline(\n",
    "    steps=[\n",
    "        (\"preprocess\", preprocess),\n",
    "        (\"model\", RandomForestClassifier(\n",
    "            n_estimators=200,\n",
    "            max_depth=None,\n",
    "            n_jobs=-1,\n",
    "            random_state=42\n",
    "        )),\n",
    "    ]\n",
    ")\n",
    "\n",
    "rf_clf.fit(X_train, y_train)\n",
    "y_pred_rf = rf_clf.predict(X_test)\n",
    "\n",
    "acc_rf = accuracy_score(y_test, y_pred_rf)\n",
    "cm_rf = confusion_matrix(y_test, y_pred_rf)\n",
    "\n",
    "print(\"\\n=== Random Forest ===\")\n",
    "print(\"Accuracy:\", acc_rf)\n",
    "print(\"Confusion matrix:\\n\", cm_rf)\n",
    "print(classification_report(y_test, y_pred_rf))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cdcc4bd-983c-4e78-bc94-d3ba7ab53d11",
   "metadata": {},
   "source": [
    "## 2.6 XGBoost – Baseline Model\n",
    "\n",
    "We now build a baseline XGBoost model using the same preprocessed features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "890bee3b-4da4-4e3d-ad93-d0d4e5b6e043",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== XGBoost (baseline) ===\n",
      "Accuracy: 0.5416666666666666\n",
      "Confusion matrix:\n",
      " [[ 58 155]\n",
      " [ 98 241]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.37      0.27      0.31       213\n",
      "           1       0.61      0.71      0.66       339\n",
      "\n",
      "    accuracy                           0.54       552\n",
      "   macro avg       0.49      0.49      0.49       552\n",
      "weighted avg       0.52      0.54      0.52       552\n",
      "\n"
     ]
    }
   ],
   "source": [
    "xgb_base = Pipeline(\n",
    "    steps=[\n",
    "        (\"preprocess\", preprocess),\n",
    "        (\"model\", XGBClassifier(\n",
    "            objective=\"binary:logistic\",\n",
    "            eval_metric=\"logloss\",\n",
    "            tree_method=\"hist\",   # fast on CPU\n",
    "            random_state=42,\n",
    "        )),\n",
    "    ]\n",
    ")\n",
    "\n",
    "xgb_base.fit(X_train, y_train)\n",
    "y_pred_xgb_base = xgb_base.predict(X_test)\n",
    "\n",
    "acc_xgb_base = accuracy_score(y_test, y_pred_xgb_base)\n",
    "cm_xgb_base = confusion_matrix(y_test, y_pred_xgb_base)\n",
    "\n",
    "print(\"=== XGBoost (baseline) ===\")\n",
    "print(\"Accuracy:\", acc_xgb_base)\n",
    "print(\"Confusion matrix:\\n\", cm_xgb_base)\n",
    "print(classification_report(y_test, y_pred_xgb_base))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c1ed4f-d686-4039-97ed-fe9f52edddb2",
   "metadata": {},
   "source": [
    "## 2.7 XGBoost – Hyperparameter Tuning\n",
    "\n",
    "We perform hyperparameter tuning for XGBoost using `RandomizedSearchCV`:\n",
    "\n",
    "- `n_estimators`\n",
    "- `max_depth`\n",
    "- `learning_rate`\n",
    "- `subsample`\n",
    "- `colsample_bytree`\n",
    "- `min_child_weight`\n",
    "- `gamma`\n",
    "\n",
    "This helps improve model performance beyond the baseline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "623e9708-3637-48ca-9f6d-e0caa8d86ade",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n",
      "Best CV accuracy: 0.6100543478260869\n",
      "Best params: {'model__subsample': 0.7, 'model__n_estimators': 400, 'model__min_child_weight': 7, 'model__max_depth': 4, 'model__learning_rate': 0.01, 'model__gamma': 0, 'model__colsample_bytree': 1.0}\n"
     ]
    }
   ],
   "source": [
    "xgb_clf = Pipeline(\n",
    "    steps=[\n",
    "        (\"preprocess\", preprocess),\n",
    "        (\"model\", XGBClassifier(\n",
    "            objective=\"binary:logistic\",\n",
    "            eval_metric=\"logloss\",\n",
    "            tree_method=\"hist\",\n",
    "            random_state=42,\n",
    "        )),\n",
    "    ]\n",
    ")\n",
    "\n",
    "param_dist = {\n",
    "    \"model__n_estimators\":    [200, 400, 600, 800],\n",
    "    \"model__max_depth\":       [3, 4, 5, 6, 8],\n",
    "    \"model__learning_rate\":   [0.01, 0.05, 0.1, 0.2],\n",
    "    \"model__subsample\":       [0.7, 0.8, 0.9, 1.0],\n",
    "    \"model__colsample_bytree\":[0.7, 0.8, 0.9, 1.0],\n",
    "    \"model__min_child_weight\":[1, 3, 5, 7],\n",
    "    \"model__gamma\":           [0, 0.1, 0.2],\n",
    "}\n",
    "\n",
    "xgb_search = RandomizedSearchCV(\n",
    "    estimator=xgb_clf,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=20,           # 可以根据时间改小或改大\n",
    "    scoring=\"accuracy\",\n",
    "    cv=3,\n",
    "    verbose=1,\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "xgb_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best CV accuracy:\", xgb_search.best_score_)\n",
    "print(\"Best params:\", xgb_search.best_params_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a10f21-8eb4-4394-9521-c39d1d7b3e31",
   "metadata": {},
   "source": [
    "## 2.8 Evaluate Best XGBoost Model on Test Set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f231faf9-623b-42d3-9088-6ba9acad0a65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Best XGBoost (tuned) on Test Set ===\n",
      "Test accuracy: 0.6177536231884058\n",
      "Confusion matrix:\n",
      " [[ 39 174]\n",
      " [ 37 302]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.51      0.18      0.27       213\n",
      "           1       0.63      0.89      0.74       339\n",
      "\n",
      "    accuracy                           0.62       552\n",
      "   macro avg       0.57      0.54      0.51       552\n",
      "weighted avg       0.59      0.62      0.56       552\n",
      "\n"
     ]
    }
   ],
   "source": [
    "best_xgb = xgb_search.best_estimator_\n",
    "\n",
    "y_pred_best = best_xgb.predict(X_test)\n",
    "acc_best = accuracy_score(y_test, y_pred_best)\n",
    "cm_best = confusion_matrix(y_test, y_pred_best)\n",
    "\n",
    "print(\"=== Best XGBoost (tuned) on Test Set ===\")\n",
    "print(\"Test accuracy:\", acc_best)\n",
    "print(\"Confusion matrix:\\n\", cm_best)\n",
    "print(classification_report(y_test, y_pred_best))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6fd90eb-025e-4bda-a02d-4372d9ebcb32",
   "metadata": {},
   "source": [
    "## 2.9 Save the Best Model\n",
    "\n",
    "We save the tuned XGBoost pipeline to disk so that it can be used later in the **prediction demo** notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1959d89f-1455-4f72-b4cc-040887857304",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved best XGBoost model to: ./Model\\best_tennis_xgb.pkl\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "MODEL_DIR = \"./Model\"\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "\n",
    "MODEL_PATH = os.path.join(MODEL_DIR, \"best_tennis_xgb.pkl\")\n",
    "joblib.dump(best_xgb, MODEL_PATH)\n",
    "\n",
    "print(\"Saved best XGBoost model to:\", MODEL_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c0bdfaf6-0149-409c-9907-61097d751929",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# XGBoost + 预处理 的 pipeline\n",
    "xgb_clf = Pipeline(\n",
    "    steps=[\n",
    "        (\"preprocess\", preprocess),\n",
    "        (\"model\", XGBClassifier(\n",
    "            objective=\"binary:logistic\",\n",
    "            eval_metric=\"logloss\",   # 避免 warning\n",
    "            tree_method=\"hist\",      # CPU 下也比较快\n",
    "            random_state=42,\n",
    "        )),\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "83ffc795-b158-4276-8234-81679e114f7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n",
      "Best CV accuracy: 0.6100543478260869\n",
      "Best params: {'model__subsample': 0.7, 'model__n_estimators': 400, 'model__min_child_weight': 7, 'model__max_depth': 4, 'model__learning_rate': 0.01, 'model__gamma': 0, 'model__colsample_bytree': 1.0}\n"
     ]
    }
   ],
   "source": [
    "# 超参数搜索空间（可以根据时间大小调节范围）\n",
    "param_dist = {\n",
    "    \"model__n_estimators\":     [200, 400, 600, 800],\n",
    "    \"model__max_depth\":        [3, 4, 5, 6, 8],\n",
    "    \"model__learning_rate\":    [0.01, 0.05, 0.1, 0.2],\n",
    "    \"model__subsample\":        [0.7, 0.8, 0.9, 1.0],\n",
    "    \"model__colsample_bytree\": [0.7, 0.8, 0.9, 1.0],\n",
    "    \"model__min_child_weight\": [1, 3, 5, 7],\n",
    "    \"model__gamma\":            [0, 0.1, 0.2],\n",
    "}\n",
    "\n",
    "xgb_search = RandomizedSearchCV(\n",
    "    estimator=xgb_clf,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=20,              # 尝试 20 组参数（可以改小 10 或改大 30）\n",
    "    scoring=\"accuracy\",     # 用 accuracy 作为评分\n",
    "    cv=3,                   # 3 折交叉验证\n",
    "    verbose=1,\n",
    "    n_jobs=-1,              # 用所有 CPU 核\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "# 开始在训练集上做调参\n",
    "xgb_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best CV accuracy:\", xgb_search.best_score_)\n",
    "print(\"Best params:\", xgb_search.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "38647315-a1de-4362-b4e4-c066c50a71b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Best XGBoost (tuned) on Test Set ===\n",
      "Test accuracy: 0.6177536231884058\n",
      "Confusion matrix:\n",
      " [[ 39 174]\n",
      " [ 37 302]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.51      0.18      0.27       213\n",
      "           1       0.63      0.89      0.74       339\n",
      "\n",
      "    accuracy                           0.62       552\n",
      "   macro avg       0.57      0.54      0.51       552\n",
      "weighted avg       0.59      0.62      0.56       552\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "# 取出最好的那个 pipeline（已经包含预处理 + XGB）\n",
    "best_xgb = xgb_search.best_estimator_\n",
    "\n",
    "y_pred_best = best_xgb.predict(X_test)\n",
    "acc_best = accuracy_score(y_test, y_pred_best)\n",
    "cm_best = confusion_matrix(y_test, y_pred_best)\n",
    "\n",
    "print(\"=== Best XGBoost (tuned) on Test Set ===\")\n",
    "print(\"Test accuracy:\", acc_best)\n",
    "print(\"Confusion matrix:\\n\", cm_best)\n",
    "print(classification_report(y_test, y_pred_best))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "521c963a-db0f-4229-9fe4-41af1a643a55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved best XGBoost model to: ./Model\\best_tennis_xgb.pkl\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "import os\n",
    "\n",
    "MODEL_DIR = \"./Model\"\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "\n",
    "MODEL_PATH = os.path.join(MODEL_DIR, \"best_tennis_xgb.pkl\")\n",
    "joblib.dump(best_xgb, MODEL_PATH)\n",
    "\n",
    "print(\"Saved best XGBoost model to:\", MODEL_PATH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2649a97c-a31e-430b-b110-cb90511fb274",
   "metadata": {},
   "source": [
    "### Model Comparison\n",
    "\n",
    "| Model                             | Test Accuracy |\n",
    "|----------------------------------|---------------|\n",
    "| Baseline (always stronger wins)  | **0.614**     |\n",
    "| Decision Tree                    | 0.525         |\n",
    "| Random Forest                    | 0.567         |\n",
    "| XGBoost (baseline)               | 0.542         |\n",
    "| **XGBoost (tuned)**              | **0.618**     |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba9d2178-e9af-4cc4-8e46-8d9b4b276bdd",
   "metadata": {},
   "source": [
    "## 2.X Model Evaluation – Metrics and Error Analysis\n",
    "\n",
    "### 2.X.1 Metrics (tuned XGBoost)\n",
    "\n",
    "For the final model, we use the tuned XGBoost classifier with the following hyperparameters:\n",
    "\n",
    "- `n_estimators = 400`\n",
    "- `max_depth = 4`\n",
    "- `learning_rate = 0.01`\n",
    "- `subsample = 0.7`\n",
    "- `colsample_bytree = 1.0`\n",
    "- `min_child_weight = 7`\n",
    "- `gamma = 0`\n",
    "\n",
    "On the held-out test set (20% of the data), the model achieves:\n",
    "\n",
    "- **Accuracy:** ≈ **0.618**\n",
    "\n",
    "The confusion matrix on the test set is:\n",
    "\n",
    "\\[\n",
    "\\begin{bmatrix}\n",
    "39 & 174 \\\\\n",
    "37 & 302\n",
    "\\end{bmatrix}\n",
    "\\]\n",
    "\n",
    "where rows are the **true labels** and columns are the **predicted labels**:\n",
    "\n",
    "- Row 0: `stronger_win = 0`  (upset: lower-ranked player wins)  \n",
    "- Row 1: `stronger_win = 1`  (normal: higher-ranked player wins)\n",
    "\n",
    "So the entries are:\n",
    "\n",
    "- **True Negative (TN) = 39**  \n",
    "  - True label = 0 (upset), prediction = 0  \n",
    "  - Matches where an upset occurs and the model correctly predicts an upset.\n",
    "\n",
    "- **False Positive (FP) = 174**  \n",
    "  - True label = 0 (upset), prediction = 1  \n",
    "  - Matches where an upset occurs, but the model incorrectly predicts that the stronger player will win.  \n",
    "  - These are **missed upsets**.\n",
    "\n",
    "- **False Negative (FN) = 37**  \n",
    "  - True label = 1 (stronger player wins), prediction = 0  \n",
    "  - Matches where the stronger player actually wins, but the model incorrectly predicts an upset.  \n",
    "  - These are **false upset alarms**.\n",
    "\n",
    "- **True Positive (TP) = 302**  \n",
    "  - True label = 1 (stronger player wins), prediction = 1  \n",
    "  - Matches where the stronger player wins and the model predicts this correctly.\n",
    "\n",
    "From the classification report, we also observe:\n",
    "\n",
    "- For class 1 (**stronger player wins**):  \n",
    "  - Recall ≈ **0.89** → the model correctly identifies most matches where the stronger player wins.  \n",
    "- For class 0 (**upset**):  \n",
    "  - Recall ≈ **0.18** → the model only detects a small fraction of actual upsets.\n",
    "\n",
    "---\n",
    "\n",
    "### 2.X.2 Explanation of Model Errors (False Positives & False Negatives)\n",
    "\n",
    "In this problem, the positive class is:\n",
    "\n",
    "- `stronger_win = 1` → higher-ranked player wins (the **expected** outcome)\n",
    "\n",
    "and the negative class is:\n",
    "\n",
    "- `stronger_win = 0` → an upset occurs (lower-ranked player wins)\n",
    "\n",
    "From this perspective:\n",
    "\n",
    "#### False Positives (FP = 174) – Missed Upsets\n",
    "\n",
    "- These are matches where **an upset actually happened**, but the model still predicted that the stronger player would win.\n",
    "- In other words, the model is **too optimistic about the stronger player**, and fails to anticipate that the underdog will win.\n",
    "- This is not surprising, because:\n",
    "  - The EDA showed that, as the ranking gap increases, the stronger player’s win rate can easily reach 70–90%.  \n",
    "  - Most of the features (ranking gap, age difference, surface, level) are more informative for “normal wins” than for rare upsets.\n",
    "- In practical terms, FPs mean that the model **misses surprising matches**.  \n",
    "  If we used this model for betting or upset prediction, these are exactly the matches where we fail to see the upset coming.\n",
    "\n",
    "#### False Negatives (FN = 37) – False Upset Alarms\n",
    "\n",
    "- These are matches where the **stronger player actually won**, but the model predicted an upset.\n",
    "- Compared to FP, the number of FN is much smaller (37 vs 174), consistent with the high recall for class 1 (≈ 0.89).\n",
    "- FN usually happen when:\n",
    "  - The ranking gap is small (almost equal strength),  \n",
    "  - Or contextual factors (surface, tournament level) suggest higher upset risk.\n",
    "- In practice, FNs correspond to cases where the model is **too pessimistic about the stronger player**, flagging a potential upset that does not occur.\n",
    "\n",
    "---\n",
    "\n",
    "### 2.X.3 Interpretation\n",
    "\n",
    "Overall, the tuned XGBoost model:\n",
    "\n",
    "- Achieves a test accuracy of **about 0.618**, slightly better than the naive baseline that always predicts the stronger player to win (≈ 0.614).\n",
    "- Correctly classifies most matches where the stronger player wins (high recall for class 1).\n",
    "- Struggles to predict **rare upsets**, which is reflected by the large number of **false positives (missed upsets)** and the low recall for class 0.\n",
    "\n",
    "This behavior is consistent with the EDA findings:\n",
    "\n",
    "- Ranking gap is a very strong predictor of match outcomes.\n",
    "- Professional tennis is inherently dominated by stronger players, and true upsets are relatively rare and hard to forecast even with historical data.\n",
    "\n",
    "In summary, the model is very good at confirming what we already know (the stronger player usually wins),  \n",
    "and provides limited but non-zero ability to flag potential upsets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb1341f-5850-4c99-ada5-080bff45e2f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
